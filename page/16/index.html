<!DOCTYPE HTML>

<html>

<head>
	<meta charset="utf-8">
	<title>Playground for the mind</title>
	<meta name="author" content="Volkan Paksoy">

	
	<meta name="description" content="
		
	">
	

	<!-- http://t.co/dKP3o1e -->
	<meta name="HandheldFriendly" content="True">
	<meta name="MobileOptimized" content="320">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="https://feeds.feedburner.com/PlaygroundForTheMind" rel="alternate" title="Playground for the mind" type="application/atom+xml">
	
	<link rel="canonical" href="https://volkanpaksoy.com/page/16/">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:300,400,400italic,600,700,800' rel='stylesheet' type='text/css'>
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link rel="stylesheet" href="/css/code.css" type="text/css">
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script src="/js/slash.js" async></script>
	<script src="/js/GithubRepoWidget.min.js" async></script>

  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5266062674794996"
  crossorigin="anonymous"></script>

  

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y3V7V2XX9Q"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Y3V7V2XX9Q');
</script>

<!-- <script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'G-Y3V7V2XX9Q']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script> -->



  <!-- Twitter cards -->
  <meta name="twitter:site"    content="@volkan_paksoy">
  <meta name="twitter:creator" content="@Volkan Paksoy">
  <meta name="twitter:title"   content="">

  
  <meta name="twitter:description" content="">
  

  
  <meta name="twitter:card"  content="summary">
  <meta name="twitter:image" content=""> -->
  
  <!-- end of Twitter cards -->


</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner"><div class="profilepic">
	
	<a href="/">
		<img src="https://s.gravatar.com/avatar/760bf518408c38ebb0f0e5adb8089e30?s=80?s=160" alt="Profile Picture" style="width: 160px;">
	</a>	
	
</div>
<hgroup>
  <h1 class="site-title" style="font-size: 2.4em;">
		<a style="color: rgba(255, 165, 0, 0.8);" href="/">Playground for the mind</a>
	</h1>
  
    <h2>It's all about the journey, not the destination</h2>
  
</hgroup>

<nav id="sub-nav">
	<div class="social">
		
			<a class="rss" href="https://feeds.feedburner.com/PlaygroundForTheMind" title="RSS" target="_blank" rel="noopener noreferrer">RSS</a>
		
		
			<a class="github" href="https://github.com/volkanpaksoy" title="GitHub" target="_blank" rel="noopener noreferrer">GitHub</a>
		
		
			<a class="stackoverflow" href="https://stackoverflow.com/users/3093396/volkan-paksoy" title="StackOverflow" target="_blank" rel="noopener noreferrer">StackOverflow</a>
		
	</div>
</nav>

<nav id="main-nav">
<ul class="main-navigation">
  <li><a href="/archive">posts</a></li>
  <li><a href="/category">categories</a></li>
</ul>
<br>
<br>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5266062674794996" crossorigin="anonymous"></script>
<!-- vp.com - default -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5266062674794996" data-ad-slot="6750480330" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</nav></header>				
			</div>
		</div>
		<div class="mid-col">
			<div class="mid-col-container">
				<div id="content" class="inner">
<div itemscope itemtype="http://schema.org/Blog">



    <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
        
	<h1 class="title" itemprop="name"><a href="/archive/2017/11/30/Backing-up-GitHub-Account-with-PowerShell/" itemprop="url">Backing up GitHub Account with PowerShell</a></h1>
	<div class="meta">
	<span class="date">




<time datetime="2017-11-30T06:00:00+00:00" itemprop="datePublished">November 30, 2017</time></span>
	<span class="categories"><!-- <span class="category-link"><a href="/category/devops">devops</a></span> -->

<!-- devops -->




    <a href="/category/devops">devops</a>
    
</span>
	<span class="tags">git, powershell</span>
	
</div>
	<div class="entry-content" itemprop="articleBody">
		<p>Having lots of projects and assets stored on GitHub I thought it might be a good idea to create periodical backups of my entire GitHub account (all repositories, both public and private). The beauty of it is since Git is open source, this way I can migrate my account to anywhere and even host it on my own server on AWS.</p>

<h2 id="challenges">Challenges</h2>
<p>With the above goal in mind, I started to outline what’s necessary to achieve this task:</p>

<ol>
  <li>Automate calling GitHub API to get all repos including private ones. (Of course one should be aware of <a href="https://developer.github.com/v3/#rate-limiting" target="_blank" rel="noopener noreferrer">GitHub API rate limits</a> which is currently 5000 requests per hour. If you use up all your allowance with your scripts you may not be able to use it yourself. Good thing is they are returning how many calls are left before you exceed your quota in <em>x-ratelimit-remaining</em> HTTP header in their responses.)</li>
  <li>Pull all the latest versions for all branches. Overwrite local versions in cases of conflict.</li>
  <li>Find a way to easily transfer a git repository (A compressed single file version rather than individual files) if transferring to another medium is required (such as an S3 bucket)</li>
</ol>

<p>With these challenges ahead, I first started looking into getting the repos from GitHub:</p>

<h2 id="consuming-github-api-via-powershell">Consuming GitHub API via PowerShell</h2>
<p>First, I shopped around for existing libraries for this task (such as <a href="https://github.com/PowerShell/PowerShellForGitHub" target="_blank" rel="noopener noreferrer">PowerShellForGitHub</a> by Microsoft but it didn’t work for me. Basically I couldn’t even manage the samples on their Wiki. It kept giving cmdlet not found error so I gave up.)</p>

<p>Found a nice <a href="https://channel9.msdn.com/Blogs/trevor-powershell/Automating-the-GitHub-REST-API-Using-PowerShell" target="_blank" rel="noopener noreferrer">video on Channel 9</a> about consuming REST APIs via PowerShell which uses GitHub API as a case study. It was perfect for me as my goal was to use GitHub API anyway. And since this is a generic approach to consume APIs it can come handy in the future as well. It’s quite easy using basic authentication.</p>

<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5266062674794996" crossorigin="anonymous"></script>

<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5266062674794996" data-ad-slot="9650311114"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script></p>

<h3 id="authorization">Authorization</h3>
<p>First step, is to create a <a href="https://github.com/settings/tokens" target="_blank" rel="noopener noreferrer">Personal Access Token</a> with repo scope. (Make sure to copy the value before you close the page, there is no way to retrieve it afterwards.)</p>

<p>After the access token has been obtained, I had to generate authorization header as shown in the Channel 9 video:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$token = '&lt;YOUR GITHUB ACCOUNT NAME&gt;:&lt;PERSONAL ACCESS TOKEN&gt;'
$base64Token = [System.Convert]::ToBase64String([char[]]$token)
$headers = @{
    Authorization = 'Basic {0}' -f $base64Token
};

$response = Invoke-RestMethod -Headers $headers -Uri https://api.github.com/user/repos
</code></pre></div></div>

<p>This way I was able to get the repositories including the private ones but by default it returns 30 records on a page so I had to traverse over the pages .</p>

<h3 id="handling-pagination">Handling pagination</h3>
<p>GitHub sends the next and the last page URLs in link header:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;https://api.github.com/user/repos?page=2&gt;; rel="next", &lt;https://api.github.com/user/repos?page=3&gt;; rel="last"
</code></pre></div></div>

<p>The challenge here is that 	looks like Invoke-RestMethod response doesn’t allow to access headers which is a huge bummer as there are useful info in the headers as shown in the screenshot:</p>

<p><img src="/images/vpblogimg/2017/11/github-api-response-headers.png" alt="GitHub response headers in Postman"></p>

<p>At this point, I wanted to use <a href="https://github.com/pcgeek86/PSGitHub" target="_blank" rel="noopener noreferrer">PSGitHub</a> mentioned in the video but as of this writing it doesn’t support getting all repositories. In fact in a note it says “<em>We need to figure out how to handle data pagination</em>” which made me think we are on the same page here (no pun intended!)</p>

<p>GitHub supports a page size parameter (e.g. per_page=50) but the documentation says the maximum value is 100. Although it is tempting to use that one as that would bring all my repos and leave some room for the future ones as well I wanted to go with a more permanent solution. So I decided to request more pages as longs as there are objects returning like this</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$page = 1

Do
{
    $response = Invoke-RestMethod -Headers $headers -Uri "https://api.github.com/user/repos?page=$page"
    
    foreach ($obj in $response)
    {
        Write-Host ($obj.id)
    }
    
    $page = $page + 1
}
While ($response.Count -gt 0)
</code></pre></div></div>

<p>Now in the foreach loop of course  I have to do something with the repo information instead of just printing the id.</p>

<h3 id="cloning--pulling-repositories">Cloning / pulling repositories</h3>
<p>At this point I was able to get all my repositories. GitHub API only handles account information so now I needed to able to run actual git commands to get my code.</p>

<p>First I had installed PowerShell on Mac which is quite simple as specified in the documentation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew tap caskroom/cask
brew cask install powershell
</code></pre></div></div>

<p>With Git already installed on my machine, all is left was using Git commands to clone or update repo on PowerShell terminal such as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git fetch --all
git reset --hard origin/master
</code></pre></div></div>

<p>Since this is just going to be a backup copy I don’t want to deal with merge conflicts and just overwriting everything local.</p>

<p>Another approach could be deleting the old repo and cloning it from scratch but I think this would be a bit wasteful to do it everytime for each and every repository.</p>

<h2 id="putting-it-all-together">Putting it all together</h2>
<p>Now that I have all the bits and pieces I have glue them together in a meaningful script than can be scheduled and here it is:</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/0309dd02703c6b5d9a73859ac2abf2d8.js"> </script>

<h2 id="conclusion-and-future-improvements">Conclusion and Future Improvements</h2>
<p>This version accomplishes the basic task of backing up an entire GitHub account but it can be improved in a few ways. Maybe I can post a follow up article including those improvements. A few ideas come to mind are:</p>

<ul>
  <li>Get Gists (private and public) as well.</li>
  <li>Add option to exclude repos by name or by type (i.e. get only private ones or get all except repo123)</li>
  <li>Add an option to export them to a “non-git” medium such as an S3 bucket using git bundle (which turns out to be a great tool to pack everything in a repository in a single file)</li>
  <li>Create a Docker image that contains all the necessary software (Git, PowerShell, backup script etc) so that it can be distributed without any setup requirements.</li>
</ul>

<h2 id="resources">Resources</h2>
<ul>
  <li><a href="https://channel9.msdn.com/Blogs/trevor-powershell/Automating-the-GitHub-REST-API-Using-PowerShell" target="_blank" rel="noopener noreferrer">Channel9 video on using GitHub API with PowerShell</a></li>
  <li><a href="https://developer.github.com/v3/" target="_blank" rel="noopener noreferrer">GitHub API Documentation</a></li>
  <li><a href="https://developer.github.com/v3/guides/traversing-with-pagination/" target="_blank" rel="noopener noreferrer">Traversing with Pagination</a></li>
  <li><a href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/invoke-restmethod?view=powershell-5.1" target="_blank" rel="noopener noreferrer">PowerShell Invoke-RestMethod documentation</a></li>
  <li><a href="https://github.com/pcgeek86/PSGitHub" target="_blank" rel="noopener noreferrer">PSGitHub Repository</a></li>
  <li>​<a href="https://github.com/PowerShell/PowerShell/blob/master/docs/installation/linux.md#macos-1012" target="_blank" rel="noopener noreferrer">Installing PowerShell on Mac</a>
</li>
</ul>

		
		
	</div>


    </article>




    <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
        
	<h1 class="title" itemprop="name"><a href="/archive/2017/11/20/Backing-up-MySQL-To-AWS-S3-with-Powershell/" itemprop="url">Backing Up MySQL to AWS S3 with PowerShell</a></h1>
	<div class="meta">
	<span class="date">




<time datetime="2017-11-20T06:00:00+00:00" itemprop="datePublished">November 20, 2017</time></span>
	<span class="categories"><!-- <span class="category-link"><a href="/category/awsdevops">aws</a></span> -->

<!-- aws, devops -->




    <a href="/category/aws">aws</a>
    , 

    <a href="/category/devops">devops</a>
    
</span>
	<span class="tags">s3, mysql, powershell</span>
	
</div>
	<div class="entry-content" itemprop="articleBody">
		<p>I have an application that uses MySQL database. Because of cost concerns it’s running on an EC2 instance instead of RDS. As it’s not a managed environment, the burden of backing up my data falls on me. This is a small step by step guide that details how I’m backing up my MySQL database to AWS S3 with PowerShell</p>

<h2 id="part-01---aws-setup">PART 01 - AWS SETUP</h2>

<ol>
  <li>Create a bucket named (e.g. “application-backups”) on AWS S3 using AWS Management Console.</li>
  <li>Create a new IAM user (e.g “upload-backup-to-s3”)</li>
  <li>Create a new policy using management console. The policy will only give enough permissions to put objects into a single S3 bucket:</li>
</ol>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"Version"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2012-10-17"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"Statement"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">{</span><span class="w">
            </span><span class="nl">"Effect"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Allow"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"Action"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"s3:ListBucket"</span><span class="w">
            </span><span class="p">],</span><span class="w">
            </span><span class="nl">"Resource"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"arn:aws:s3:::xxxxxxx"</span><span class="w">
            </span><span class="p">]</span><span class="w">
        </span><span class="p">},</span><span class="w">
        </span><span class="p">{</span><span class="w">
            </span><span class="nl">"Effect"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Allow"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"Action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3:PutObject"</span><span class="p">,</span><span class="w">
            </span><span class="nl">"Resource"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
                </span><span class="s2">"arn:aws:s3:::xxxxxxx/*"</span><span class="w">
            </span><span class="p">]</span><span class="w">
        </span><span class="p">}</span><span class="w">
    </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>In S3 bucket properties copy ARN and replace the x’s above with that.</p>

<ol>
  <li>
    <p>Customize S3 bucket LifeCycle settings to determine how long you want the old logs retain in your bucket. In my case I set it to expire at 21 days which I think it’s large enough window for relevant database backups. I probably wouldn’t restore anything older than 21 days anyway.</p>
  </li>
  <li>
    <p>[Optional - For email notifications] Create an SES user by clicking SES -&gt; SMTP Settings -&gt; Create My SMTP Credentials</p>
  </li>
</ol>

<p>Make sure you don’t make the mistake I made which was creating an IAM user with a policy that can send emails. In the SMTP settings page there’s a note right below the button:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Your SMTP user name and password are not the same as your AWS access key ID and secret access key. Do not attempt to use your AWS credentials to authenticate yourself against the SMTP endpoint.
</code></pre></div></div>

<p>When you click the button it creates an IAM user basically for the secret key is 44 bytes whereas the IAM user I created had a secret key of 40 characters. Anyway, bottom line is in order to be able to send emails via SES create the user as described above and all should be fine.</p>

<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5266062674794996" crossorigin="anonymous"></script>

<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5266062674794996" data-ad-slot="9650311114"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script></p>

<h2 id="part-02---powershell-script">PART 02 - POWERSHELL SCRIPT</h2>

<ol>
  <li>
    <p>Download and install AWS Tools for Windows PowerShell (https://aws.amazon.com/powershell/)</p>
  </li>
  <li>
    <p>Create a script as shown below. In a nutshell what the script does is:</p>
  </li>
</ol>

<p>a. Execute mysqldump command (Comes with MySQL Server)</p>

<p>b. Zip the backup file (which reduces the size significantly)</p>

<p>c. Upload the zip file to S3 bucket</p>

<p>d. Send a notification email using SES</p>

<p>e. Delete the local files</p>

<p>This is the full script:</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/e75f2d861de8604868bec0897768896e.js"> </script>

<ul>
  <li>
    <p>For SQL Server databases, there’s a PowerShell cmdlet called <a href="https://docs.microsoft.com/en-us/powershell/module/sqlserver/backup-sqldatabase?view=sqlserver-ps" target="_blank" rel="noopener noreferrer">Backup-SQLDatabase</a> but for MySQL I think the most straightforward way is using mysqldump that comes with MySQL server.</p>
  </li>
  <li>
    <p>For password-protected zip files, you can take a look at <a href="https://cyber-defense.sans.org/blog/2016/06/06/powershell-7-zip-compress-archive-encryption#" target="_blank" rel="noopener noreferrer">this article</a> (I haven’t tried it myself)</p>
  </li>
</ul>

<p>Final step: Schedule the script by using Windows Task Scheduler</p>

<p>This is quite straightforward. Just create a task, schedule it to how often you want to backup your database.</p>

<p>In the actions section, enter “powershell” as “program/script” and the path of your PowerShell script as “argument” and that’s it.</p>

<h2 id="resources">Resources</h2>
<ul>
  <li><a href="https://docs.microsoft.com/en-us/powershell/module/sqlserver/backup-sqldatabase?view=sqlserver-ps" target="_blank" rel="noopener noreferrer">Backup-SQLDatabase cmdlet reference</a></li>
  <li><a href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.archive/compress-archive?view=powershell-5.1" target="_blank" rel="noopener noreferrer">Compress-Archive cmdlet reference</a></li>
  <li><a href="https://aws.amazon.com/powershell/" target="_blank" rel="noopener noreferrer">AWS Tools Download</a></li>
  <li><a href="https://www.vexasoft.com/blogs/powershell/7255220-powershell-tutorial-try-catch-finally-and-error-handling-in-powershell" target="_blank" rel="noopener noreferrer">PowerShell Tutorial – Try Catch Finally and error handling in PowerShell</a></li>
  <li><a href="http://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-credentials.html" target="_blank" rel="noopener noreferrer">AWS Documentation: Obtaining Your Amazon SES SMTP Credentials</a></li>
</ul>

		
		
	</div>


    </article>




    <article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
        
	<h1 class="title" itemprop="name"><a href="/archive/2017/09/05/DIY-Streaming-Service-with-Synology/" itemprop="url">DIY Streaming Service with Synology</a></h1>
	<div class="meta">
	<span class="date">




<time datetime="2017-09-05T07:00:00+01:00" itemprop="datePublished">September  5, 2017</time></span>
	<span class="categories"><!-- <span class="category-link"><a href="/category/hobby">hobby</a></span> -->

<!-- hobby -->




    <a href="/category/hobby">hobby</a>
    
</span>
	<span class="tags">synology, music, streaming</span>
	
</div>
	<div class="entry-content" itemprop="articleBody">
		<p>I’ve been a Spotify customer for quite long time but recently realized that I wasn’t using it enough to justify 10 quid per month. Amazon made a great offer for 4 months subscription for only £0.99 and I’m trying that out now but the quality of the service didn’t impress so far. Then it dawned on me: I already have lots of MP3s from my old archives, I have a fast internet connection and I have a Synology. Why not just build my own streaming?</p>

<h2 id="one-device-to-rule-them-all-synology">One device to rule them all: Synology</h2>
<p>Everyday I’m growing more fond of my Synology and regretting for all that time I haven’t utilized it fully.</p>

<p>For streaming audio, we need the server and client software. The server side comes with Synology: Audio Station</p>

<h3 id="the-server">The Server</h3>
<p>Using Synology Audio Station is a breeze. You simply connect to Synology over the network and copy your albums into the music folder. Try to have a cover art named as “cover.jpg” so that your albums shows nicely on the user interface.</p>

<p><img src="/images/vpblogimg/2017/09/synology-audio-folder-view.png" alt=""></p>

<h3 id="the-client">The Client</h3>
<p>Synology has a suite of iOS applications which are available in the Apple App Store. The one I’m using for audio streaming is called <strong>DS Audio</strong>.</p>

<p><img src="/images/vpblogimg/2017/09/synology-audio-on-appstore.PNG" alt=""></p>

<p>By using Synology’s Control Panel you can use a specific user for listening to music only. This way even if your account is compromised the attacker will only have read-only access to your music library.</p>

<p><img src="/images/vpblogimg/2017/09/synology-audio-user-permissions.png" alt=""></p>

<h2 id="connecting-to-the-server">Connecting to the server</h2>
<p>There are two ways of connecting to your server:</p>

<ol>
  <li>Dynamic DNS</li>
  <li>Quick Connect (QC)</li>
</ol>

<p>Dynamic DNS is a builtin functionality but you’d need a Synology account. Basically your Synology pings their server so that it can detec the IP changes.</p>

<p>QC is the way I chose to go with. It’s a proprietary technology by Synology. The nice thing about QC is when you are connected to your local network it uses the internal IP so it doesn’t use mobile data. When you’re outside it uses the external IP and connects over the Internet.</p>

<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5266062674794996" crossorigin="anonymous"></script>

<p><ins class="adsbygoogle" style="display:block; text-align:center;" data-ad-layout="in-article" data-ad-format="fluid" data-ad-client="ca-pub-5266062674794996" data-ad-slot="9650311114"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script></p>

<h2 id="features">Features</h2>
<ul>
  <li>You can download all the music you want from your own library without any limitations. There’s no limit set for manual downloads. For automatic downloads you can choose from no caching to caching everything or choose a fixed size from 250MB to 20GB.</li>
  <li>When you’re offline you don’t need to login. On login form there’s a link to Downloaded Songs so you can skip logging in and go straight to your local cache.</li>
  <li>You can pin your favourite albums to home screen.</li>
  <li>Creating a playlist or adding songs to playlists is cumbersome (on iPhone at least):
    <ul>
      <li>Select a song and tap on … next to the song</li>
      <li>Tap Add. This will add your song to the play queue.</li>
      <li>Tap on Play button on top right corner.</li>
      <li>Tap playlist icon on top right corner.</li>
      <li>Tap the same icon again which is now on top left corner to go into edit mode</li>
      <li>Now tap on the radio buttons on the left of the songs to select.</li>
      <li>When done, tap on the icon on the bottom left corner. This will open the Add to Playlist screen (finally!)</li>
      <li>Here you can choose an existing playlist or create a new one by clicking + icon.</li>
    </ul>
  </li>
</ul>

<p>Considering how easy this can be done on Spotify client this really needs to be improved.</p>

<ul>
  <li>In the library or Downloaded Songs sections, you can organise your music by Album, Artist, Composer, Genre and Folder. Of course in order for Artist/Composer/Genre classification to work you have to have your music properly tagged.</li>
  <li>The client has Radio featue which has builtin support for SHOUTCast</li>
</ul>

<p><img src="/images/vpblogimg/2017/09/synology-shoutcast.PNG" alt="SHOUTCast"></p>

<ul>
  <li>You can rate songs. There’s a built-in Top Rated playlist. By rating them you can play your favourite songs without needing them to be added to playlists which is a neat feature.</li>
</ul>

<p><img src="/images/vpblogimg/2017/09/synology-rate-playlist.PNG" alt=""></p>

<h2 id="conclusion">Conclusion</h2>
<p>I think having full control over my own music is great and even though DS Audio client has some drawbacks it’s worth it as it’s completely free. Also you can just set it up as a secondary streaming service in addition to your favourite paid one just in case so that you have a backup solution.</p>

<h2 id="resources">Resources</h2>
<ul>
  <li><a href="https://blog.synology.com/?p=2283" target="_blank" rel="noopener noreferrer">Synology Blog: QuickConnect, Quickly Explained</a></li>
</ul>


		
		
	</div>


    </article>


</div>
<nav id="pagenavi">
    
        <a href="/page/15" class="prev">Prev</a>
    
    
        <a href="/page/17" class="next">Next</a>
    
    <div class="center"><a href="/archive">Blog Archives</a></div>
</nav>
</div>
			</div>
			<footer id="footer" class="inner"><p>
  Copyright © 2022 - Volkan Paksoy Blog content licensed under the Creative Commons <a href="http://creativecommons.org/licenses/by/2.5/" target="_blank" rel="noopener noreferrer">CC BY 2.5</a> | Site design based on the <a href="https://github.com/shashankmehta/greyshade" target="_blank" rel="noopener noreferrer">Greyshade theme</a> under the <a href="http://sm.mit-license.org/" target="_blank" rel="noopener noreferrer">MIT license</a>
</p>
</footer>
		</div>
	</div>
</body>
</html>
